{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0tbHHvlTduS"
   },
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d933f0f-13b7-4070-8b48-83bb4a09eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os, sys, platform, importlib, math, random, json\n",
    "import numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from scipy.sparse import csgraph\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyZd28rjTduU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup & Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97453f4f-37bd-4ce0-b8ae-76bd7fc98b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===  Environment ===\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _ver(m):\n",
    "    try: return importlib.import_module(m).__version__\n",
    "    except Exception: return \"n/a\"\n",
    "print({\n",
    "    \"python\": platform.python_version(),\n",
    "    \"numpy\": _ver(\"numpy\"),\n",
    "    \"pandas\": _ver(\"pandas\"),\n",
    "    \"torch\": _ver(\"torch\"),\n",
    "    \"torch_geometric\": _ver(\"torch_geometric\"),\n",
    "    \"sklearn\": _ver(\"sklearn\"),\n",
    "    \"scipy\": _ver(\"scipy\"),\n",
    "    \"networkx\": _ver(\"networkx\"),\n",
    "})\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Check (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BREZgFv2Xl5n",
    "outputId": "57f01b71-66ff-437a-d0db-5e8ba28ddcdb"
   },
   "outputs": [],
   "source": [
    "# Simple dependency report (optional)\n",
    "import importlib, textwrap\n",
    "for lib in [\"networkx\",\"numpy\",\"pandas\",\"torch\",\"torch_geometric\",\"matplotlib\",\"sklearn\",\"scipy\",\"optuna\",\"pyvis\",\"IPython\"]:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"{lib}: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"{lib}: MISSING ({e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAe-bFRgTduX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data and Network Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfQVcjTfg2-A"
   },
   "source": [
    "## Sociometric Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build/load sociometric graph so downstream cells have G_Sociometric\n",
    "\n",
    "# Option A: from a CSV edgelist\n",
    "# Expect a file 'sociometric_edges.csv' with columns 'u','v' (participant IDs)\n",
    "# edges_df = pd.read_csv('sociometric_edges.csv', dtype=str)\n",
    "# G_Sociometric = nx.from_pandas_edgelist(edges_df, source='u', target='v', create_using=nx.Graph())\n",
    "\n",
    "# Option B: from an existing DataFrame in memory named `edges_df` (dtype=str, columns 'u','v')\n",
    "# G_Sociometric = nx.from_pandas_edgelist(edges_df, source='u', target='v', create_using=nx.Graph())\n",
    "\n",
    "if 'G_Sociometric' not in globals():\n",
    "    raise RuntimeError(\"Define G_Sociometric (e.g., from sociometric_edges.csv) before proceeding.\")\n",
    "\n",
    "# Current graph stats (feature alignment occurs in Graph Construction cell)\n",
    "print(\"Graph:\", G_Sociometric.number_of_nodes(), \"nodes;\", G_Sociometric.number_of_edges(), \"edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVf_GzADVl6p",
    "outputId": "229dcda4-1bb9-40c0-97e4-790a8f4e4437"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "node_properties_all = pd.read_csv('baseline_features.csv', low_memory=False)\n",
    "\n",
    "# Ensure the \"record_id\" column is treated as a string\n",
    "node_properties_all[\"record_id\"] = node_properties_all[\"record_id\"].astype(str)\n",
    "\n",
    "# Check the number of columns\n",
    "num_columns = node_properties_all.shape[1]\n",
    "print(f\"Number of columns in the CSV file: {num_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e539a-7cd0-47b0-ad82-197abae18175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocessing ===\n",
    "# assumes node_properties_all (rows align to participant nodes) already loaded\n",
    "id_cols = [c for c in node_properties_all.columns if c.lower() in {\"record_id\", \"id\", \"node_id\"}]\n",
    "feats_df = node_properties_all.drop(columns=id_cols, errors=\"ignore\")\n",
    "miss = feats_df.isna().mean()\n",
    "keep_cols = miss[miss <= 0.05].index.tolist()\n",
    "X = feats_df[keep_cols]\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"mean\")),\n",
    "                     (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "X_mat = preprocessor.fit_transform(X)\n",
    "X_np = (X_mat.toarray() if hasattr(X_mat, \"toarray\") else np.asarray(X_mat)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3ac3d-526e-4eb7-8ec0-261492f57d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Graph Construction ===\n",
    "# assumes an undirected NetworkX graph G_Sociometric already built\n",
    "\n",
    "# Align graph nodes to available features (by 'record_id')\n",
    "valid_nodes = set(node_properties_all['record_id'].astype(str))\n",
    "G_Sociometric = G_Sociometric.subgraph(valid_nodes).copy()\n",
    "\n",
    "# Build node list and edge index in aligned order\n",
    "node_list = list(G_Sociometric.nodes())\n",
    "node_to_idx = {n: i for i, n in enumerate(node_list)}\n",
    "edges = np.array([(node_to_idx[u], node_to_idx[v]) for u, v in G_Sociometric.edges()], dtype=np.int64).T\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index, num_nodes=len(node_list))\n",
    "\n",
    "# Re-index features to match node_list order\n",
    "id_to_row = {rid: i for i, rid in enumerate(node_properties_all['record_id'].astype(str).tolist())}\n",
    "row_idx = [id_to_row[n] for n in node_list]\n",
    "X_aligned = X_np[row_idx]\n",
    "x = torch.from_numpy(X_aligned).to(torch.float32)\n",
    "assert x.shape[0] == len(node_list), 'Feature-node count mismatch after alignment'\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, num_nodes=x.shape[0])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ef182-10cd-437d-b85c-62cfebb33a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Link Splits ===\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.15, num_test=0.15,\n",
    "    is_undirected=True, split_labels=True,\n",
    "    add_negative_train_samples=True, neg_sampling_ratio=1.0\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# Encode val/test using only training adjacency\n",
    "val_data.edge_index  = train_data.edge_index\n",
    "test_data.edge_index = train_data.edge_index\n",
    "\n",
    "train_data, val_data, test_data = train_data.to(device), val_data.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17f8d6-230e-4a56-bcf8-c6739efd3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GraphSAGE Model ===\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, 128); self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = SAGEConv(128, 128);    self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = SAGEConv(128, 128)    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index); x = self.bn1(x); x = F.relu(x); x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index); x = self.bn2(x); x = F.relu(x); x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        return (z[src] * z[dst]).sum(dim=-1)  \n",
    "\n",
    "    @staticmethod\n",
    "    def decode_all(z):  \n",
    "        return torch.sigmoid(z @ z.T)\n",
    "\n",
    "    def get_regularization_loss(self): \n",
    "        return 0.0\n",
    "\n",
    "def _metrics(pos_logits, neg_logits):\n",
    "    y_true = np.concatenate([np.ones_like(pos_logits.cpu()), np.zeros_like(neg_logits.cpu())])\n",
    "    y_prob = torch.sigmoid(torch.cat([pos_logits, neg_logits])).detach().cpu().numpy()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    roc  = roc_auc_score(y_true, y_prob)\n",
    "    ap   = average_precision_score(y_true, y_prob)  # PR-AUC\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc_auc=roc, pr_auc=ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389424c-abf0-45e9-b0a6-00ad78ced35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train/Eval ===\n",
    "def _metrics(pos_logits, neg_logits):\n",
    "    y_true = torch.cat([torch.ones_like(pos_logits), torch.zeros_like(neg_logits)]).cpu().numpy()\n",
    "    y_prob = torch.sigmoid(torch.cat([pos_logits, neg_logits])).detach().cpu().numpy()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1)\n",
    "\n",
    "def train_one_epoch(model, data, opt):\n",
    "    model.train(); opt.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    pos = model.decode(z, data.pos_edge_label_index)\n",
    "    neg = model.decode(z, data.neg_edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        torch.cat([pos, neg]),\n",
    "        torch.cat([torch.ones_like(pos), torch.zeros_like(neg)])\n",
    "    )\n",
    "    loss.backward(); opt.step(); return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_split(model, data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    pos = model.decode(z, data.pos_edge_label_index)\n",
    "    neg = model.decode(z, data.neg_edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        torch.cat([pos, neg]),\n",
    "        torch.cat([torch.ones_like(pos), torch.zeros_like(neg)])\n",
    "    )\n",
    "    return float(loss), _metrics(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276d7c9-3dcd-4aa4-94cb-cd5568744188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HPO + Evaluation ===\n",
    "HPO_MODE = \"grid\"  \n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE   = 30\n",
    "FIXED = dict(lr=3e-4, weight_decay=1e-5, dropout=0.5)\n",
    "GRID  = {\"lr\":[3e-4, 1e-3], \"dropout\":[0.5], \"weight_decay\":[1e-5]}\n",
    "N_TRIALS = 25  \n",
    "\n",
    "def _fit(params):\n",
    "    model = GraphSAGEModel(train_data.num_features, dropout=params[\"dropout\"]).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    best = {\"vloss\": float(\"inf\"), \"state\": None}\n",
    "    patience = PATIENCE\n",
    "    for ep in range(1, MAX_EPOCHS+1):\n",
    "        _ = train_one_epoch(model, train_data, opt)\n",
    "        vloss, _vm = eval_split(model, val_data)\n",
    "        if vloss < best[\"vloss\"]:\n",
    "            best[\"vloss\"] = vloss\n",
    "            best[\"state\"] = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = PATIENCE\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0: break\n",
    "    model.load_state_dict(best[\"state\"]); return model\n",
    "\n",
    "def select_params():\n",
    "    if HPO_MODE == \"none\":\n",
    "        return FIXED\n",
    "    if HPO_MODE == \"grid\":\n",
    "        import itertools\n",
    "        best_params, best_f1, best_vloss = None, -1.0, float(\"inf\")\n",
    "        for lr, dp, wd in itertools.product(GRID[\"lr\"], GRID[\"dropout\"], GRID[\"weight_decay\"]):\n",
    "            params = {\"lr\": lr, \"dropout\": dp, \"weight_decay\": wd}\n",
    "            m = _fit(params); vloss, vm = eval_split(m, val_data)\n",
    "            if vm[\"f1\"] > best_f1 or (vm[\"f1\"] == best_f1 and vloss < best_vloss):\n",
    "                best_params, best_f1, best_vloss = params, vm[\"f1\"], vloss\n",
    "        return best_params\n",
    "    if HPO_MODE == \"optuna\":\n",
    "        import optuna\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"lr\": trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.30, 0.60),\n",
    "                \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "            }\n",
    "            m = _fit(params); _, vm = eval_split(m, val_data); return vm[\"f1\"]\n",
    "        study.optimize(objective, n_trials=N_TRIALS)\n",
    "        return study.best_params\n",
    "    raise ValueError(HPO_MODE)\n",
    "\n",
    "best_params = select_params()\n",
    "with open(\"best_params.json\",\"w\") as f: json.dump(best_params, f, indent=2)\n",
    "\n",
    "TEST_SEEDS = [13,17,19,23,29]\n",
    "rows = []\n",
    "for s in TEST_SEEDS:\n",
    "    # Set RNG seeds per iteration\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Re-split data per seed\n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=0.15, num_test=0.15,\n",
    "        is_undirected=True, split_labels=True,\n",
    "        add_negative_train_samples=True, neg_sampling_ratio=1.0\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    # Encode val/test using only training adjacency\n",
    "    val_data.edge_index  = train_data.edge_index\n",
    "    test_data.edge_index = train_data.edge_index\n",
    "    train_data, val_data, test_data = train_data.to(device), val_data.to(device), test_data.to(device)\n",
    "\n",
    "    # Fit and evaluate (metrics at fixed threshold 0.5)\n",
    "    model = _fit(best_params)\n",
    "    tloss, tm = eval_split(model, test_data)\n",
    "\n",
    "    # Record thresholds for reproducibility\n",
    "    tau_val = threshold_by_validation_f1(model, train_data, val_data)\n",
    "    prob = score_all_pairs_matrix(model, train_data.edge_index, data.x.to(device))\n",
    "    G_emp = edge_index_to_nx(test_data.num_nodes, test_data.pos_edge_label_index)\n",
    "    tau_den = threshold_by_density(prob, target_edges=G_emp.number_of_edges())\n",
    "\n",
    "    rows.append(dict(seed=s, loss=tloss, tau_val_f1=tau_val, tau_density=tau_den, **tm))\n",
    "    torch.save(model.state_dict(), f\"best_model_seed{s}.pt\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df); print(df.agg([\"mean\",\"std\"]).T)\n",
    "df.to_csv(\"test_metrics_by_seed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f446ee-a1a9-4c45-a00a-faae6863580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Spectral Similarity ===\n",
    "def _lcc(G: nx.Graph):\n",
    "    if G.number_of_nodes() == 0: return G\n",
    "    return G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
    "\n",
    "def _norm_lap_eigs(G: nx.Graph, k=None):\n",
    "    A = nx.to_scipy_sparse_array(G, format=\"csr\", dtype=float)\n",
    "    L = csgraph.laplacian(A, normed=True)\n",
    "    n = G.number_of_nodes()\n",
    "    if n == 0: return np.array([])\n",
    "    if k is None or k >= n-1:\n",
    "        w = np.linalg.eigvalsh(L.toarray())\n",
    "    else:\n",
    "        w = eigsh(L, k=k, which=\"SM\", return_eigenvectors=False)\n",
    "    return np.sort(np.real(w))\n",
    "\n",
    "def spectral_similarity(G_ref: nx.Graph, G_est: nx.Graph, k=None):\n",
    "    e1 = _norm_lap_eigs(_lcc(G_ref), k); e2 = _norm_lap_eigs(_lcc(G_est), k)\n",
    "    if len(e1) == 0 or len(e2) == 0: return np.nan\n",
    "    m = max(len(e1), len(e2))\n",
    "    e1 = np.pad(e1, (0, m-len(e1))); e2 = np.pad(e2, (0, m-len(e2)))\n",
    "    return float(np.dot(e1, e2) / (norm(e1) * norm(e2)))\n",
    "\n",
    "def structural_summary(G: nx.Graph):\n",
    "    Gc = _lcc(G)\n",
    "    deg = np.array([d for _, d in Gc.degree()])\n",
    "    return dict(\n",
    "        density = nx.density(G),\n",
    "        avg_path_len = nx.average_shortest_path_length(Gc) if Gc.number_of_nodes() > 1 else np.nan,\n",
    "        diameter = nx.diameter(Gc) if Gc.number_of_nodes() > 1 else np.nan,\n",
    "        transitivity = nx.transitivity(G),\n",
    "        deg_p01 = np.percentile(deg, 1) if deg.size else np.nan,\n",
    "        deg_p25 = np.percentile(deg, 25) if deg.size else np.nan,\n",
    "        deg_p50 = np.percentile(deg, 50) if deg.size else np.nan,\n",
    "        deg_p75 = np.percentile(deg, 75) if deg.size else np.nan,\n",
    "        deg_p99 = np.percentile(deg, 99) if deg.size else np.nan,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814055b-21d3-4b33-825f-7b830843dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers ===\n",
    "@torch.no_grad()\n",
    "def score_all_pairs_matrix(model, base_edge_index, x):\n",
    "    model.eval()\n",
    "    z = model.encode(x, base_edge_index)  # encode with training adjacency (leakage-safe)\n",
    "    logits = z @ z.T\n",
    "    prob = torch.sigmoid(logits)\n",
    "    prob.fill_diagonal_(0)\n",
    "    return prob  # NxN\n",
    "\n",
    "def threshold_by_validation_f1(model, train_data, val_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(val_data.x, train_data.edge_index)\n",
    "        pos = model.decode(z, val_data.pos_edge_label_index)\n",
    "        neg = model.decode(z, val_data.neg_edge_label_index)\n",
    "        y_true = torch.cat([torch.ones_like(pos), torch.zeros_like(neg)]).cpu().numpy()\n",
    "        y_prob = torch.sigmoid(torch.cat([pos, neg])).cpu().numpy()\n",
    "        taus = np.linspace(0.01, 0.99, 99)\n",
    "        f1s = [f1_score(y_true, (y_prob >= t).astype(int)) for t in taus]\n",
    "        return float(taus[int(np.argmax(f1s))])\n",
    "\n",
    "def threshold_by_density(prob_adj, target_edges: int):\n",
    "    triu = prob_adj.triu(1).flatten().cpu().numpy()\n",
    "    kth = max(1, int(target_edges))\n",
    "    τ = np.partition(triu, -kth)[-kth]\n",
    "    return float(τ)\n",
    "\n",
    "def edge_index_to_nx(num_nodes, edge_index):\n",
    "    G = nx.Graph(); G.add_nodes_from(range(num_nodes))\n",
    "    ei = edge_index.detach().cpu().numpy()\n",
    "    G.add_edges_from(map(tuple, ei.T))\n",
    "    return G\n",
    "\n",
    "def impute_graph_from_prob(prob_adj, τ: float):\n",
    "    keep = (prob_adj >= τ).nonzero(as_tuple=False).cpu().numpy()\n",
    "    G = nx.Graph(); G.add_nodes_from(range(prob_adj.shape[0])); G.add_edges_from(map(tuple, keep))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === External validation ===\n",
    "# Expected files: file.csv (with 'record_id'), file_edges.csv (u,v as strings)\n",
    "\n",
    "post_X = pd.read_csv('file_features.csv', low_memory=False)\n",
    "post_X['record_id'] = post_X['record_id'].astype(str)\n",
    "\n",
    "# Reuse the same fitted preprocessor (trained on baseline X)\n",
    "post_feats = post_X.drop(columns=[c for c in post_X.columns if c.lower() in {'record_id','id','node_id'}], errors='ignore')\n",
    "post_mat = preprocessor.transform(post_feats)\n",
    "post_np  = (post_mat.toarray() if hasattr(post_mat, \"toarray\") else np.asarray(post_mat)).astype(\"float32\")\n",
    "x_new    = torch.from_numpy(post_np).to(torch.float32).to(device)\n",
    "\n",
    "# Impute with attributes only (no edges)\n",
    "τ = tau_val  # or tau_den; report both\n",
    "G_imp_post = external_validate(model, x_new, τ=τ)\n",
    "\n",
    "# Build empirical post-COVID graph\n",
    "post_edges = pd.read_csv('file_edges.csv', dtype=str)\n",
    "G_post_emp = nx.from_pandas_edgelist(post_edges, 'u', 'v', create_using=nx.Graph())\n",
    "\n",
    "# Report structural similarity\n",
    "print(\"Spectral similarity (post empirical vs imputed):\", spectral_similarity(G_post_emp, G_imp_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4f893-7399-48ee-8fe6-a43817bd6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_loops_edge_index(n, device):\n",
    "    idx = torch.arange(n, device=device)\n",
    "    return torch.stack([idx, idx], dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def external_validate(model, x_new, τ: float = 0.5):\n",
    "    base_adj = self_loops_edge_index(x_new.size(0), device)\n",
    "    prob = score_all_pairs_matrix(model, base_adj, x_new)\n",
    "    return impute_graph_from_prob(prob, τ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300df45-17c0-46f7-b462-91bc452f3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one seed’s model and best params\n",
    "with open(\"best_params.json\") as f:\n",
    "    bp = json.load(f)\n",
    "model = GraphSAGEModel(train_data.num_features, dropout=bp.get(\"dropout\", 0.5)).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_seed13.pt\", map_location=device))\n",
    "\n",
    "# Thresholds for two operating points\n",
    "tau_val = threshold_by_validation_f1(model, train_data, val_data)\n",
    "prob = score_all_pairs_matrix(model, train_data.edge_index, data.x.to(device))\n",
    "G_emp = edge_index_to_nx(data.num_nodes, data.edge_index) \n",
    "tau_den = threshold_by_density(prob, target_edges=G_emp.number_of_edges())\n",
    "\n",
    "print(\"Threshold (val-F1):\", tau_val)\n",
    "print(\"Threshold (density-matched):\", tau_den)\n",
    "\n",
    "# Impute graphs at both thresholds\n",
    "G_imp_val = impute_graph_from_prob(prob, tau_val)\n",
    "G_imp_den = impute_graph_from_prob(prob, tau_den)\n",
    "# Default selection for downstream cells (e.g., plots)\n",
    "G_imp = G_imp_val\n",
    "\n",
    "print(\"Spectral similarity (empirical vs imputed, val-F1):\", spectral_similarity(G_emp, G_imp_val))\n",
    "print(\"Spectral similarity (empirical vs imputed, density):\", spectral_similarity(G_emp, G_imp_den))\n",
    "\n",
    "print(\"Empirical summary:\", structural_summary(G_emp))\n",
    "print(\"Imputed summary (val-F1):\", structural_summary(G_imp_val))\n",
    "print(\"Imputed summary (density):\", structural_summary(G_imp_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner: Hyperparameter Optimization\n",
    "import json\n",
    "print(\"HPO_MODE =\", HPO_MODE)\n",
    "\n",
    "# Only select if not already computed\n",
    "if 'best_params' not in globals():\n",
    "    best_params = select_params()\n",
    "    with open(\"best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(\"Selected best_params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify / confirm model hyperparameters for downstream cells\n",
    "# Default to tuned best_params if available; otherwise use manuscript defaults.\n",
    "params = dict(\n",
    "    lr=best_params.get(\"lr\", 3e-4) if 'best_params' in globals() else 3e-4,\n",
    "    dropout=best_params.get(\"dropout\", 0.5) if 'best_params' in globals() else 0.5,\n",
    "    weight_decay=best_params.get(\"weight_decay\", 1e-5) if 'best_params' in globals() else 1e-5,\n",
    ")\n",
    "print(\"Using params:\", params)\n",
    "\n",
    "# Optional manual override example:\n",
    "# params.update({\"lr\": 3e-4, \"dropout\": 0.5, \"weight_decay\": 1e-5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load or Train a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model checkpoint (or train one if missing)\n",
    "import os, torch\n",
    "\n",
    "SEED_TO_LOAD = 13  # change to 17, 19, 23, 29 to load other seeds\n",
    "ckpt = f\"best_model_seed{SEED_TO_LOAD}.pt\"\n",
    "\n",
    "model = GraphSAGEModel(train_data.num_features, dropout=params[\"dropout\"]).to(device)\n",
    "if os.path.exists(ckpt):\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    print(f\"Loaded checkpoint: {ckpt}\")\n",
    "else:\n",
    "    print(f\"Checkpoint {ckpt} not found; training a model with current params...\")\n",
    "    model = _fit(params)\n",
    "    torch.save(model.state_dict(), ckpt)\n",
    "    print(f\"Saved new checkpoint: {ckpt}\")\n",
    "\n",
    "# Quick sanity: evaluate on validation and test\n",
    "vloss, vm = eval_split(model, val_data)\n",
    "tloss, tm = eval_split(model, test_data)\n",
    "print(\"Val:\", vm, \"loss=\", vloss)\n",
    "print(\"Test:\", tm, \"loss=\", tloss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbpfIWqOvrRW"
   },
   "source": [
    "# Imputation and Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is designed to impute an undirected graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOfN3QK5vzm8"
   },
   "source": [
    "## Network Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ekkukYn5T9s"
   },
   "outputs": [],
   "source": [
    "# Visualize original test network\n",
    "def live_plot(cur_G, name):\n",
    "    \"\"\"Plot network visualization\"\"\"\n",
    "    from pyvis.network import Network\n",
    "    import IPython\n",
    "    net = Network(height=\"800px\", width=\"100%\", notebook=True, cdn_resources=\"remote\")\n",
    "    net.from_nx(cur_G)\n",
    "    plotname = name + \".html\"\n",
    "    net.toggle_physics(True)\n",
    "    net.force_atlas_2based(gravity=-80,central_gravity=0.005,spring_length=100,spring_strength=2,damping=0.1,overlap=1)\n",
    "    net.show(plotname)\n",
    "    return IPython.display.HTML(filename=plotname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_impute_header"
   },
   "source": [
    "# Evaluation, Spectral Similarity, and Clean Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_csv_export"
   },
   "outputs": [],
   "source": [
    "# Export per-seed test metrics to CSV (reconstruct if df missing)\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "seeds_default = [13, 17, 19, 23, 29]\n",
    "\n",
    "def _load_best_params():\n",
    "    if os.path.exists('best_params.json'):\n",
    "        with open('best_params.json') as f:\n",
    "            return json.load(f)\n",
    "    return {'dropout': 0.5, 'lr': 3e-4, 'weight_decay': 1e-5}\n",
    "\n",
    "params = _load_best_params()\n",
    "\n",
    "# Rebuild df if not present\n",
    "if 'df' not in globals():\n",
    "    rows = []\n",
    "    for s in seeds_default:\n",
    "        ckpt = f'best_model_seed{s}.pt'\n",
    "        if not os.path.exists(ckpt):\n",
    "            continue\n",
    "        # Reconstruct the split and RNG state for this seed\n",
    "        random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(s)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        transform = RandomLinkSplit(\n",
    "            num_val=0.15, num_test=0.15,\n",
    "            is_undirected=True, split_labels=True,\n",
    "            add_negative_train_samples=True, neg_sampling_ratio=1.0\n",
    "        )\n",
    "        train_data, val_data, test_data = transform(data)\n",
    "        val_data.edge_index  = train_data.edge_index\n",
    "        test_data.edge_index = train_data.edge_index\n",
    "        train_data, val_data, test_data = train_data.to(device), val_data.to(device), test_data.to(device)\n",
    "        m = GraphSAGEModel(train_data.num_features, dropout=params.get('dropout', 0.5)).to(device)\n",
    "        m.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "        tloss, tm = eval_split(m, test_data)\n",
    "        tau_val = threshold_by_validation_f1(m, train_data, val_data)\n",
    "        prob = score_all_pairs_matrix(m, train_data.edge_index, data.x.to(device))\n",
    "        G_emp = edge_index_to_nx(data.num_nodes, data.edge_index) \n",
    "        tau_den = threshold_by_density(prob, target_edges=G_emp.number_of_edges())\n",
    "        rows.append(dict(seed=s, loss=tloss, tau_val_f1=tau_val, tau_density=tau_den, **tm))\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "if 'df' in globals() and not df.empty:\n",
    "    df.to_csv('test_metrics_by_seed.csv', index=False)\n",
    "    print('Wrote test_metrics_by_seed.csv with', len(df), 'rows')\n",
    "else:\n",
    "    print('Warning: no seed metrics available to write; run the HPO/eval cell first.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deg_histogram"
   },
   "outputs": [],
   "source": [
    "# Degree distribution histogram for the imputed graph\n",
    "import matplotlib.pyplot as plt\n",
    "degs = [d for _, d in G_imp.degree()]\n",
    "plt.figure(); plt.hist(degs, bins=30); plt.xlabel('Degree'); plt.ylabel('Count'); plt.title('Imputed graph degree distribution'); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
